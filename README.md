# CW2_of_TTDS
## 1. 信息检索评估（IR Evaluation）
步骤：
### a. 下载并理解数据文件：
- 下载 system_results.csv 和 qrels.csv。
- 理解文件格式：
  - system_results.csv：包含6个不同IR系统对10个查询的检索结果。
    - 格式：system_number,query_number,doc_number,rank_of_doc,score
  - qrels.csv：包含每个查询的相关文档列表及其相关性得分。
    - 格式：query_id,doc_id,relevance
### b. 开发评估模块（EVAL）：
- 计算以下评估指标：
  - P@10：前10个检索结果的精确度。
  - R@50：前50个检索结果的召回率。
  - r-precision：基于相关文档数量的精确度。
  - AP（平均精确度）：考虑检索结果的整体精确度。
    - 提示：对于上述指标，所有相关文档的相关性视为1，不区分1、2、3。
  - nDCG@10 和 nDCG@20：归一化折损累积增益，考虑文档的相关性等级。
    - 注意：必须使用Lecture 9中的公式，其他实现方式不被接受。
### c. 生成评估结果文件：
- 创建 ir_eval.csv 文件，格式如下：
system_number,query_number,P@10,R@50,r-precision,AP,nDCG@10,nDCG@20
1,1,0.000,0.000,0.000,0.000,0.000,0.000
...
1,mean,0.000,0.000,0.000,0.000,0.000,0.000
...
6,10,0.000,0.000,0.000,0.000,0.000,0.000
6,mean,0.000,0.000,0.000,0.000,0.000,0.000

- 每个系统（1-6）和每个查询（1-10）的评估结果，以及每个系统的平均值（query_number 为 mean）。
- 确保所有得分保留三位小数。
- 使用提供的Python脚本检查文件格式是否正确。
### d. 撰写报告的“IR Evaluation”部分：
- 描述最佳系统：
  - 基于平均得分，确定每个评估指标下的最佳系统。
  - 例如，哪个系统在P@10上表现最佳。
- 统计显著性分析：
  - 判断最佳系统是否在统计上显著优于第二名系统。
  - 使用双尾t检验，p值为0.05。
  - 解释分析结果和原因。

---
## 2. 文本分析（Text Analysis）
步骤：
### a. 数据准备：
- 下载训练语料库，包括《古兰经》、《旧约》和《新约》的经文。
- 每行格式：corpus_name<TAB>text。
### b. 数据预处理：
- 常规预处理步骤：
  - 分词。
  - 转换为小写。
  - 去除停用词和标点符号。
  - 其他必要的清洗步骤（如词干提取或词形还原）。
### c. 计算统计量：
- 计算互信息（MI）和卡方检验（χ²）得分：
  - 对每个语料库，计算每个词的MI和χ²得分。
  - 生成格式为 token,score 的排序列表。
### d. 撰写报告的“Token Analysis”部分：
- 比较排名差异：
  - 分析MI和χ²方法产生的词汇排名差异。
- 语料库特征分析：
  - 从排名中提取关于三个语料库的特征和差异。
- 结果展示：
  - 包含一个表格，展示每个语料库中，每种方法排名前10的词汇。
### e. 主题模型（LDA）分析：
- 运行LDA模型：
  - 将所有语料库的经文合并，视为一个整体语料库。
  - 设置主题数 k=20，运行LDA模型。
- 计算主题平均得分：
  - 对于每个语料库，计算每个主题的平均文档-主题概率。
- 识别最相关主题：
  - 对于每个语料库，找到平均得分最高的主题。
  - 获取这些主题中概率最高的前10个词汇。
### f. 撰写报告的“Topic Analysis”部分：
- 结果展示：
  - 一个表格，列出每个语料库最相关主题的前10个词及其概率。
- 主题命名：
  - 为每个主题起1-3个词的标签。
- 模型分析：
  - LDA模型揭示的语料库信息。
  - 是否存在两个语料库共有的主题，且第三个没有。
  - 这些主题的高概率词汇示例。
  - 与MI和χ²分析结果的比较。

---
## 3. 文本分类（Text Classification）
步骤：
### a. 数据准备：
- 下载带有情感标签的推文数据集。
- 数据集划分：
  - 将数据随机打乱。
  - 划分为训练集和开发集（如90%训练，10%开发）。
### b. 建立基线模型：
- 特征提取：
  - 提取词袋（BOW）特征。
- 模型训练：
  - 使用SVM分类器，参数C=1000。
  - 训练模型以预测情感标签（正面、负面、中性）。
### c. 模型评估：
- 计算评估指标：
  - 对每个类别，计算精确度、召回率和F1-score。
  - 计算宏平均的精确度、召回率和F1-score。
- 在训练集和开发集上评估：
  - 只使用训练集进行训练。
  - 在开发集上评估模型性能。
### d. 错误分析：
- 选择错误实例：
  - 从开发集中挑选3个被错误分类的实例。
- 撰写报告的“Classification”部分：
  - 提供这3个实例。
  - 分析并假设错误分类的原因。
### e. 改进模型：
- 基于错误分析进行改进：
  - 修改预处理步骤（如去除或保留停用词）。
  - 特征选择（如使用MI得分最高的特征）。
  - 调整模型参数（如改变C值）。
  - 尝试不同的分类算法。
- 验证改进效果：
  - 在开发集上评估改进后的模型。
  - 确认性能相对于基线有所提升。
### f. 在测试集上评估：
- 获取测试集：
  - 测试集将于11月25日发布。
- 模型训练和评估：
  - 使用训练集重新训练模型。
  - 不再对模型进行任何修改。
  - 在测试集上评估模型。
- 生成结果文件：
  - 创建 classification.csv，格式如下：
system,split,p-pos,r-pos,f-pos,p-neg,r-neg,f-neg,p-neu,r-neu,f-neu,p-macro,r-macro,f-macro
baseline,train,...
baseline,dev,...
baseline,test,...
improved,train,...
improved,dev,...
improved,test,...
  - 填写对应的评估指标值。
  - 使用提供的脚本检查文件格式是否正确。
### g. 撰写报告的“Classification”部分：
- 改进方法说明：
  - 详细解释为提高性能所做的改进。
  - 说明选择这些改进方法的原因。
- 性能提升分析：
  - 提及在开发集和测试集上宏F1-score的提升幅度。
  - 如果测试集结果与开发集有显著差异，讨论可能的原因。
